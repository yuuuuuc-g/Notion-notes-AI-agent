import json
import os
from dotenv import load_dotenv
from llm_client import get_completion
from web_ops import fetch_url_content
import notion_ops  # å¯¼å…¥å®Œå…¨ä½“çš„ notion_ops

try:
    from file_ops import read_pdf_content
except ImportError:
    read_pdf_content = None

load_dotenv()

# --- ğŸ§  Brain A: Classifier ---
def classify_intent(text):
    prompt = f"""
    Analyze the content type. First 800 chars: {text[:800]}
    Return JSON: {{ "type": "Spanish" }} OR {{ "type": "General" }}
    """
    response = get_completion(prompt)
    if "Spanish" in response: return {"type": "Spanish"}
    return {"type": "General"}

# --- ğŸ§  Brain B: Spanish Logic ---
def check_topic_match(new_text, existing_pages):
    """æŸ¥é‡é€»è¾‘ (é€šç”¨)"""
    if not existing_pages:
        return {"match": False}
        
    titles_str = "\n".join([f"ID: {p['id']}, Title: {p['title']}" for p in existing_pages])
    prompt = f"""
    Library check.
    Existing Notes: {titles_str}
    New Content: {new_text[:800]}
    
    Task: Check if the new content belongs to an existing topic/book/note.
    Output JSON: {{ "match": true, "page_id": "...", "page_title": "..." }} OR {{ "match": false }}
    """
    try:
        # ç›´æ¥è§£æï¼Œä¸ä½¿ç”¨ parse_json åŒ…è£…
        return json.loads(get_completion(prompt).replace("```json", "").replace("```", "").strip())
    except:
        return {"match": False}

def generate_spanish_content(text):
    """
    è¥¿è¯­æ¨¡å¼ï¼šä¿æŒç»“æ„åŒ–è¾“å‡º
    """
    prompt = f"""
    You are a Spanish teacher. Process this content: {text[:15000]}
    
    Output JSON with this structure (No Markdown code blocks):
    {{
        "title": "Note Title",
        "category": "Vocabulary/Listening/Grammar",
        "summary": "Chinese summary",
        "blocks": [
            {{ "type": "heading", "content": "1. Core Vocabulary" }},
            {{ 
                "type": "table", 
                "content": {{
                    "headers": ["Spanish", "Chinese", "Example"],
                    "rows": [
                        ["Word1", "Meaning1", "Ex1"],
                        ["Word2", "Meaning2", "Ex2"]
                    ]
                }}
            }},
            {{ "type": "heading", "content": "2. Key Sentences" }},
            {{ "type": "list", "content": ["Sentence 1", "Sentence 2"] }}
        ]
    }}
    """
    response = get_completion(prompt)
    clean_json = response.replace("```json", "").replace("```", "").strip()
    try:
        return json.loads(clean_json)
    except:
        return None

def decide_merge_strategy(new_text, structure_text, tables):
    """å†³ç­–ï¼šè¥¿è¯­è¡¨æ ¼åˆå¹¶ç­–ç•¥"""
    prompt = f"""
    Editor logic.
    Structure: {structure_text}
    Tables: {json.dumps(tables)}
    New Content: {new_text[:1000]}
    Output JSON: {{ "action": "insert_row", "table_id": "...", "row_data": ["Col1", "Col2", "Col3"] }} OR {{ "action": "append_text" }}
    """
    try:
        return json.loads(get_completion(prompt).replace("```json", "").replace("```", "").strip())
    except:
        return {"action": "append_text"}

# --- ğŸ§  Brain C: General Logic (å·²å¢å¼ºï¼šè¯¦å°½æ¨¡å¼) ---
def process_general_knowledge(text):
    """
    é€šç”¨æ¨¡å¼ï¼šå¤§å¹…å¢åŠ äº†æå–çš„è¯¦ç»†ç¨‹åº¦
    """
    prompt = f"""
    You are a professional research assistant. 
    Analyze the following content deeply: {text[:20000]} 
    
    **CRITICAL INSTRUCTION**: 
    Do NOT summarize too briefly. I need detailed, comprehensive notes.
    Capture ALL the nuance, logic, and technical details from the source.
    
    Output strictly JSON:
    {{
        "title": "Chinese Title (Clear & Professional)",
        "summary": "Chinese Summary (300-500 words). Be detailed. Cover the context, problem, and solution.",
        "tags": ["Tag1", "Tag2", "Tag3"],
        "key_points": [
            "Point 1: Detailed explanation (100+ words) of the first key concept...",
            "Point 2: Detailed explanation of the second concept...",
            ... (Extract 8-15 key points. Be exhaustive.)
        ]
    }}
    """
    response = get_completion(prompt)
    clean_json = response.replace("```json", "").replace("```", "").strip()
    try:
        return json.loads(clean_json)
    except:
        return None

# --- ğŸ© Main Workflow (Proç‰ˆï¼šåŒå‘æŸ¥é‡) ---
def main_workflow(user_input=None, uploaded_file=None):
    processed_text = ""
    original_url = None
    
    # 1. è·å–è¾“å…¥
    if uploaded_file and read_pdf_content:
        print("ğŸ“‚ File detected...")
        processed_text = read_pdf_content(uploaded_file)
    elif user_input:
        if user_input.strip().startswith("http"):
            original_url = user_input.strip()
            print(f"ğŸŒ Fetching URL: {original_url}")
            processed_text = fetch_url_content(original_url)
            processed_text = f"[Source] {original_url}\n{processed_text}"
        else:
            processed_text = user_input
    
    if not processed_text:
        print("âš ï¸ Empty input")
        return

    # 2. è·¯ç”±
    print("ğŸš¦ Routing content...")
    intent = classify_intent(processed_text)
    content_type = intent.get('type', 'General')
    print(f"ğŸ‘‰ Type: {content_type}")

    # 3. å¤„ç†æµç¨‹
    if content_type == 'Spanish':
        print("ğŸ‡ªğŸ‡¸ Spanish Mode Activated...")
        # A. æŸ¥é‡ (è¥¿è¯­åº“)
        existing_titles = notion_ops.get_all_page_titles(notion_ops.DB_SPANISH_ID)
        match = check_topic_match(processed_text, existing_titles)
        
        if match.get('match'):
            # B. åˆå¹¶é€»è¾‘
            page_id = match.get('page_id')
            title = match.get('page_title')
            print(f"ğŸ’¡ Merging with existing note: {title}")
            
            structure, tables = notion_ops.get_page_structure(page_id)
            if tables:
                strategy = decide_merge_strategy(processed_text, structure, tables)
                if strategy.get('action') == 'insert_row':
                    notion_ops.add_row_to_table(strategy['table_id'], strategy['row_data'])
                    return 
            
            # è¿½åŠ æ–‡æœ¬
            data = generate_spanish_content(processed_text)
            if data:
                notion_ops.append_to_page(page_id, data.get('summary'), data.get('blocks'))
        else:
            # C. æ–°å»ºé€»è¾‘
            print("ğŸ†• Creating New Spanish Note...")
            data = generate_spanish_content(processed_text)
            if data:
                notion_ops.create_study_note(
                    data.get('title'),
                    data.get('category', 'General'),
                    data.get('summary'),
                    data.get('blocks'), 
                    original_url
                )

    else:
        print("ğŸŒ General Knowledge Mode...")
        
        # === âœ¨ æ–°å¢åŠŸèƒ½ï¼šé€šç”¨æ¨¡å¼æŸ¥é‡ ===
        # 1. è·å–é€šç”¨åº“çš„æ‰€æœ‰æ ‡é¢˜
        existing_titles = notion_ops.get_all_page_titles(notion_ops.DB_GENERAL_ID)
        
        # 2. æ£€æŸ¥æ˜¯å¦é‡å¤
        match = check_topic_match(processed_text, existing_titles)
        
        # 3. ç”Ÿæˆå†…å®¹ (ç°åœ¨æ˜¯è¯¦å°½ç‰ˆ)
        print("ğŸ§  Generating comprehensive notes...")
        data = process_general_knowledge(processed_text)
        
        if not data:
            print("âŒ Content generation failed.")
            return

        if match.get('match'):
            # === å‘½ä¸­é‡å¤ï¼šè¿½åŠ å†…å®¹ ===
            page_id = match.get('page_id')
            title = match.get('page_title')
            print(f"ğŸ’¡ Topic Exists! Merging into: ã€Š{title}ã€‹")
            
            # è°ƒç”¨ append_to_page (notion_ops ä¼šè‡ªåŠ¨å¤„ç† list ç±»å‹çš„ key_points)
            notion_ops.append_to_page(
                page_id, 
                data.get('summary'), 
                data.get('key_points') # è¿™é‡Œä¼ å…¥çš„æ˜¯å­—ç¬¦ä¸²åˆ—è¡¨
            )
        else:
            # === æ— é‡å¤ï¼šæ–°å»ºé¡µé¢ ===
            print("ğŸ†• Topic is new. Creating General Note...")
            notion_ops.create_general_note(data, original_url)

    print("âœ… Processing Complete!")