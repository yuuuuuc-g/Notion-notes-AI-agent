import json
import re
import os
from dotenv import load_dotenv
from llm_client import get_completion
from web_ops import fetch_url_content
from notion_ops import (
    create_study_note,          
    create_general_note,        
    get_all_page_titles, 
    append_to_page, 
    get_page_structure, 
    add_row_to_table
)

load_dotenv()

# è·å–ä¸¤ä¸ªæ•°æ®åº“çš„ ID
DB_SPANISH = os.getenv("NOTION_DATABASE_ID") 
DB_GENERAL = os.getenv("NOTION_DATABASE_ID_GENERAL") 

# --- è¾…åŠ©å‡½æ•° ---
def parse_json(text):
    cleaned_text = re.sub(r"```json|```", "", text).strip()
    try: return json.loads(cleaned_text)
    except: return None

def read_input_file():
    file_path = "note.txt"
    if not os.path.exists(file_path):
        with open(file_path, "w", encoding="utf-8") as f: f.write("è¯·åœ¨æ­¤ç²˜è´´ç¬”è®°æˆ–URL")
        return None
    with open(file_path, "r", encoding="utf-8") as f: return f.read().strip()

# --- ğŸ§  æ ¸å¿ƒå¤§è„‘ï¼šåˆ†ç±»å™¨ (Router) ---
def classify_intent(text):
    prompt = f"""
    ä½ æ˜¯ä¸€ä¸ªå†…å®¹åˆ†å‘æ€»ç›‘ã€‚è¯·åˆ†æä»¥ä¸‹æ–‡æœ¬çš„å†…å®¹ç±»å‹ã€‚
    
    ã€æ–‡æœ¬å‰800å­—ã€‘
    {text[:800]}
    
    ã€åˆ¤æ–­é€»è¾‘ã€‘
    1. **Spanish**: å†…å®¹åŒ…å«è¥¿ç­ç‰™è¯­å•è¯ã€è¯­æ³•è®²è§£ï¼Œæˆ–è€…çœ‹èµ·æ¥åƒæ˜¯ä¸€æ®µè¥¿è¯­æ•™å­¦è§†é¢‘çš„å­—å¹•ã€‚
    2. **General**: å®è§‚ç»æµã€æ”¿æ²»ã€AI æŠ€æœ¯ã€ç¼–ç¨‹ä»£ç ã€èŒåœºæ„Ÿæ‚Ÿç­‰éè¯­è¨€å­¦ä¹ ç±»ã€‚
    
    ã€è¾“å‡º JSONã€‘
    {{ "type": "Spanish" }} æˆ– {{ "type": "General" }}
    """
    result = get_completion(prompt)
    return parse_json(result)

# --- ğŸ§  å¤§è„‘ Bï¼šé€šç”¨å†…å®¹å¤„ç†å™¨ ---
def process_general_knowledge(text):
    prompt = f"""
    ä½ æ˜¯ä¸€ä¸ªèµ„æ·±çš„ç ”ç©¶å‘˜ã€‚è¯·æ•´ç†è¿™ä»½ç´ æï¼Œæå–æ ¸å¿ƒæ´å¯Ÿã€‚
    
    è¾“å…¥å†…å®¹ï¼š
    {text[:12000]} 
    
    ä»»åŠ¡ï¼š
    1. æ‹Ÿå®šä¸€ä¸ªå¸å¼•äººçš„æ ‡é¢˜ã€‚
    2. æå– 3-5 ä¸ªå…³é”®æ ‡ç­¾ (Tags)ã€‚
    3. å†™ä¸€æ®µ 100-200 å­—çš„æ·±åº¦æ‘˜è¦ã€‚
    4. å°†æ­£æ–‡æ•´ç†ä¸ºæ¸…æ™°çš„ç¬”è®°ç»“æ„ (Blocks)ï¼Œä¿ç•™æ•°æ®ã€ä»£ç å’Œè®ºç‚¹ã€‚
    
    è¾“å‡º JSON: {{ "title": "...", "tags": [], "summary": "...", "blocks": [...] }}
    """
    return get_completion(prompt)

# --- ğŸ§  å¤§è„‘ Aï¼šè¥¿è¯­å¤„ç†å™¨ (å‡çº§ç‰ˆ) ---
def check_topic_match(new_text, existing_pages):
    titles_list = [f"ID: {p['id']}, Title: {p['title']}" for p in existing_pages]
    titles_str = "\n".join(titles_list) if titles_list else "æš‚æ— ç°æœ‰ç¬”è®°"
    
    prompt = f"""
    ä½ æ˜¯ä¸€ä¸ªæ™ºèƒ½å›¾ä¹¦ç®¡ç†å‘˜ã€‚
    ã€ç°æœ‰è¥¿è¯­ç¬”è®°åˆ—è¡¨ã€‘
    {titles_str}
    ã€æ–°ç´ æå‰1000å­—ã€‘
    {new_text[:1000]}
    ã€ä»»åŠ¡ã€‘åˆ¤æ–­æ–°ç´ æçš„æ ¸å¿ƒä¸»é¢˜æ˜¯å¦å·²å­˜åœ¨ï¼Ÿ
    è¾“å‡º JSON: 
    {{ "match": true, "page_id": "...", "page_title": "..." }} 
    æˆ– 
    {{ "match": false, "suggested_title": "..." }}
    """
    return get_completion(prompt)

def generate_spanish_content(raw_text):
    """
    æ ¸å¿ƒç”Ÿæˆé€»è¾‘ï¼šé’ˆå¯¹é•¿å­—å¹•è¿›è¡Œäº†ä¸“é—¨ä¼˜åŒ–
    """
    prompt = f"""
    ä½ æ˜¯ä¸€ä¸ªç²¾é€šè¥¿ç­ç‰™è¯­æ•™å­¦çš„é«˜çº§ç¼–è¾‘ã€‚
    ç”¨æˆ·çš„è¾“å…¥å¯èƒ½æ˜¯ä¸€ä»½ç¬”è®°ï¼Œä¹Ÿå¯èƒ½æ˜¯ä¸€æ®µé•¿è§†é¢‘çš„ã€å­—å¹•åŸæ–‡ã€‘ã€‚
    
    ã€è¾“å…¥å†…å®¹ã€‘
    {raw_text[:15000]}  <-- å…³é”®ä¿®æ”¹ï¼šè¯»å–å‰ 15000 å­—ï¼Œç¡®ä¿è¯»åˆ°å®Œæ•´å†…å®¹ï¼
    
    ã€ä½ çš„ä»»åŠ¡ã€‘
    ä¸è¦åªæ˜¯æ€»ç»“ï¼å¿…é¡»è¿›è¡Œ**â€œçŸ¥è¯†èƒå–â€**ã€‚
    
    1. **è¯†åˆ«æ ¸å¿ƒè¯æ±‡**ï¼šå¦‚æœè¾“å…¥æ˜¯å­—å¹•ï¼Œå¿…é¡»ä»ä¸­æå–å‡ºè‡³å°‘ 5-10 ä¸ªæ ¸å¿ƒç”Ÿè¯/çŸ­è¯­ï¼Œåˆ¶ä½œæˆè¡¨æ ¼ã€‚
    2. **æå–åœ°é“ä¾‹å¥**ï¼šæ‰¾åˆ°åŸæ–‡æœ¬ä¸­å‡ºç°çš„ç²¾å½©å¥å­ï¼Œä¿ç•™è¥¿è¯­åŸæ–‡å¹¶é™„å¸¦ä¸­æ–‡è§£é‡Šã€‚
    3. **ç»“æ„åŒ–é‡ç»„**ï¼šä½¿ç”¨ Heading, List, Table å°†å†…å®¹æ’ç‰ˆã€‚
    
    ã€JSON è¾“å‡ºç»“æ„ã€‘
    {{
        "title": "ç²¾å‡†çš„æ ‡é¢˜ (å¦‚æœæ˜¯è§†é¢‘ï¼Œç”¨è§†é¢‘ä¸»é¢˜)",
        "category": "å¬åŠ›/è¯æ±‡/è¯­æ³•",
        "summary": "200å­—å·¦å³çš„å†…å®¹ç®€ä»‹ï¼Œæ¦‚æ‹¬è§†é¢‘ä¸»è¦åœºæ™¯å’Œè¯é¢˜",
        "blocks": [
            {{ "type": "heading", "content": "1. æ ¸å¿ƒè¯æ±‡ (Vocabulario)" }},
            {{ 
                "type": "table", 
                "content": {{
                    "headers": ["è¥¿è¯­", "ä¸­æ–‡å«ä¹‰", "å¤‡æ³¨/åœºæ™¯"],
                    "rows": [
                        ["el cajero", "æ”¶é“¶å‘˜", "å•†åœºç»“è´¦åœºæ™¯"],
                        ["pagar con tarjeta", "åˆ·å¡æ”¯ä»˜", "å¸¸ç”¨çŸ­è¯­"]
                    ]
                }}
            }},
            {{ "type": "heading", "content": "2. ç²¾å½©ä¾‹å¥ (Frases Clave)" }},
            {{ "type": "list", "content": ["Me gustarÃ­a probarme esta camisa. (æˆ‘æƒ³è¯•è¯•è¿™ä»¶è¡¬è¡«)", "Â¿Tienen este pantalÃ³n en talla M? (è¿™æ¡è£¤å­æœ‰Mç å—?)"] }},
            {{ "type": "heading", "content": "3. è¯­æ³•/æ–‡åŒ–è¦ç‚¹" }},
            {{ "type": "text", "content": "è¿™é‡Œæ”¾è¯­æ³•è§£ææˆ–æ–‡åŒ–èƒŒæ™¯..." }}
        ]
    }}
    
    åªè¾“å‡º JSONã€‚
    """
    return get_completion(prompt)

def decide_merge_strategy(new_text, existing_structure_text, available_tables):
    prompt = f"""
    ä½ æ˜¯ä¸€ä¸ªæ™ºèƒ½ç¼–è¾‘ã€‚
    ã€é¡µé¢ç»“æ„ã€‘{existing_structure_text}
    ã€ç°æœ‰è¡¨æ ¼ã€‘{json.dumps(available_tables)}
    ã€æ–°å†…å®¹ã€‘{new_text[:2000]}
    ä»»åŠ¡ï¼šåˆ¤æ–­æ˜¯å¦é€‚åˆæ’å…¥ç°æœ‰è¡¨æ ¼ï¼Ÿ
    è¾“å‡º JSON: {{ "action": "insert_row", "table_id": "...", "row_data": [...] }} æˆ– {{ "action": "append_text" }}
    """
    return get_completion(prompt)

# --- ğŸ© æ€»æŒ‡æŒ¥é€»è¾‘ ---
def main_workflow(raw_input):
    original_url = None
    
    # 1. è¯†åˆ« URL
    url_pattern = re.compile(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')
    if url_pattern.match(raw_input.strip()):
        original_url = raw_input.strip()
        print(f"ğŸŒ æ­£åœ¨æŠ“å– URL: {original_url}")
        content = fetch_url_content(original_url)
        if not content: return
        # æ˜¾å¼ä¿ç•™ URL ä¿¡æ¯ä¾›åç»­ä½¿ç”¨
        processed_text = f"ã€æ¥æº URLã€‘{original_url}\n\n{content}"
    else:
        processed_text = raw_input

    # 2. ğŸš¦ è·¯ç”±åˆ†ç±»
    intent = classify_intent(processed_text)
    content_type = intent.get('type', 'General')
    print(f"ğŸ‘‰ åˆ¤å®šç±»å‹ä¸ºï¼šã€{content_type}ã€‘")

    # === é€šé“ A: è¥¿è¯­å­¦ä¹  ===
    if content_type == 'Spanish':
        
        # ä¸ºäº†æ¼”ç¤ºç®€å•ï¼Œè¿™é‡Œå‡è®¾ all_pages ä»…æ¥è‡ªè¥¿è¯­åº“
        # å®é™…é¡¹ç›®ä¸­åº”ä¼ å…¥ DB_SPANISH
        all_pages = get_all_page_titles() 
        
        match_result = parse_json(check_topic_match(processed_text, all_pages))
        
        if match_result and match_result.get('match'):
            page_id = match_result.get('page_id')
            page_title = match_result.get('page_title', 'æœªçŸ¥æ ‡é¢˜')
            print(f"ğŸ’¡ èåˆæ—§ç¬”è®°: ã€Š{page_title}ã€‹")
            
            if not page_id: return

            structure_text, tables = get_page_structure(page_id)
            
            if tables:
                merge_decision = parse_json(decide_merge_strategy(processed_text, structure_text, tables))
                if merge_decision and merge_decision.get('action') == 'insert_row':
                    print("ğŸ’¡ ç­–ç•¥ï¼šæ’å…¥è¡¨æ ¼...")
                    add_row_to_table(merge_decision['table_id'], merge_decision['row_data'])
                else:
                    print("ğŸ’¡ ç­–ç•¥ï¼šæ–‡æœ«è¿½åŠ ...")
                    full_content = parse_json(generate_spanish_content(processed_text))
                    if full_content:
                        append_to_page(page_id, full_content['summary'], full_content['blocks'])
            else:
                full_content = parse_json(generate_spanish_content(processed_text))
                if full_content:
                    append_to_page(page_id, full_content['summary'], full_content['blocks'])
        else:
            print("ğŸ†• æ–°å»ºè¥¿è¯­ç¬”è®°...")
            full_content = parse_json(generate_spanish_content(processed_text))
            if full_content:
                create_study_note(
                    full_content['title'], 
                    full_content.get('category', 'å¬åŠ›'), # å­—å¹•é€šå¸¸å½’ç±»ä¸ºå¬åŠ›
                    full_content['summary'], 
                    full_content['blocks']
                )

    # === é€šé“ B: é€šç”¨çŸ¥è¯† ===
    else:
        print("ğŸŒ è¿›å…¥é€šç”¨çŸ¥è¯†æ¨¡å¼ (AI/ç»æµ/ç¼–ç¨‹)...")
        if not DB_GENERAL:
            print("âŒ é”™è¯¯ï¼šæœªé…ç½®é€šç”¨æ•°æ®åº“ ID")
            return

        analysis = parse_json(process_general_knowledge(processed_text))
        if analysis:
            create_general_note(
                title=analysis.get('title', 'æœªå‘½åæ–‡æ¡£'),
                tags=analysis.get('tags', []),
                summary=analysis.get('summary', ''),
                url=original_url,
                content_blocks=analysis.get('blocks', []),
                db_id=DB_GENERAL
            )

if __name__ == "__main__":
    raw = read_input_file()
    if raw and "è¯·åœ¨æ­¤ç²˜è´´" not in raw:
        main_workflow(raw)
        print("\nğŸ‰ å¤„ç†å®Œæˆï¼")