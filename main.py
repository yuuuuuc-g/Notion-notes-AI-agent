import json
import re
import os
from file_ops import read_pdf_content
from dotenv import load_dotenv
from llm_client import get_completion
from web_ops import fetch_url_content
from notion_ops import (
    create_study_note,          
    create_general_note,        
    get_all_page_titles, 
    append_to_page, 
    get_page_structure, 
    add_row_to_table
)

load_dotenv()

# è·å–ä¸¤ä¸ªæ•°æ®åº“çš„ ID
DB_SPANISH = os.getenv("NOTION_DATABASE_ID") 
DB_GENERAL = os.getenv("NOTION_DATABASE_ID_GENERAL") 

# --- è¾…åŠ©å‡½æ•° ---
def parse_json(text):
    cleaned_text = re.sub(r"```json|```", "", text).strip()
    try: return json.loads(cleaned_text)
    except: return None

def read_input_file():
    file_path = "note.txt"
    if not os.path.exists(file_path):
        with open(file_path, "w", encoding="utf-8") as f: f.write("è¯·åœ¨æ­¤ç²˜è´´ç¬”è®°æˆ–URL")
        return None
    with open(file_path, "r", encoding="utf-8") as f: return f.read().strip()

# --- ğŸ§  æ ¸å¿ƒå¤§è„‘ï¼šåˆ†ç±»å™¨ (Router) ---
def classify_intent(text):
    prompt = f"""
    ä½ æ˜¯ä¸€ä¸ªå†…å®¹åˆ†å‘æ€»ç›‘ã€‚è¯·åˆ†æä»¥ä¸‹æ–‡æœ¬çš„å†…å®¹ç±»å‹ã€‚
    
    ã€æ–‡æœ¬å‰800å­—ã€‘
    {text[:800]}
    
    ã€åˆ¤æ–­é€»è¾‘ã€‘
    1. **Spanish**: å†…å®¹åŒ…å«è¥¿ç­ç‰™è¯­å•è¯ã€è¯­æ³•è®²è§£ï¼Œæˆ–è€…çœ‹èµ·æ¥åƒæ˜¯ä¸€æ®µè¥¿è¯­æ•™å­¦è§†é¢‘çš„å­—å¹•ã€‚
    2. **General**: å®è§‚ç»æµã€æ”¿æ²»ã€AI æŠ€æœ¯ã€ç¼–ç¨‹ä»£ç ã€èŒåœºæ„Ÿæ‚Ÿç­‰éè¯­è¨€å­¦ä¹ ç±»ã€‚
    
    ã€è¾“å‡º JSONã€‘
    {{ "type": "Spanish" }} æˆ– {{ "type": "General" }}
    """
    result = get_completion(prompt)
    return parse_json(result)

# --- ğŸ§  å¤§è„‘ Bï¼šé€šç”¨å†…å®¹å¤„ç†å™¨ (å‡çº§ç‰ˆ) ---
def process_general_knowledge(text):
    """
    å¤„ç†é€šç”¨æ–‡ç« /è§†é¢‘ï¼šç”Ÿæˆæ‘˜è¦ã€æ ‡ç­¾ã€æ ‡é¢˜ã€æ ¸å¿ƒçŸ¥è¯†ç‚¹
    """
    prompt = f"""
    ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„çŸ¥è¯†ç®¡ç†åŠ©æ‰‹ã€‚è¯·åˆ†æä»¥ä¸‹å†…å®¹ï¼ˆå¯èƒ½æ˜¯æ–‡ç« ã€è§†é¢‘å­—å¹•æˆ–æŠ€æœ¯æ–‡æ¡£ï¼‰ï¼š
    
    {text[:15000]} 
    
    è¯·æå–ä»¥ä¸‹ä¿¡æ¯å¹¶ä»¥ä¸¥æ ¼çš„ JSON æ ¼å¼è¾“å‡ºï¼š
    1. title: ä¸€ä¸ªç®€çŸ­ç²¾ç‚¼çš„æ ‡é¢˜ï¼ˆä¸­æ–‡ï¼‰ã€‚
    2. summary: 200å­—ä»¥å†…çš„ç²¾è¾Ÿæ‘˜è¦ï¼Œæ¦‚æ‹¬æ ¸å¿ƒæ€æƒ³ã€‚
    3. tags: 3-5ä¸ªç›¸å…³æ ‡ç­¾ï¼ˆArray of stringsï¼‰ã€‚
    4. key_points: æå– 3-7 ä¸ªæ ¸å¿ƒçŸ¥è¯†ç‚¹æˆ–å¹²è´§ï¼ˆArray of stringsï¼‰ã€‚
       - å¦‚æœæ˜¯ä»£ç /æŠ€æœ¯æ–‡ç« ï¼Œè¯·æ€»ç»“æ ¸å¿ƒé€»è¾‘æˆ–å…³é”®å‡½æ•°ã€‚
       - å¦‚æœæ˜¯è§‚ç‚¹æ–‡ç« ï¼Œè¯·æ€»ç»“æ ¸å¿ƒè®ºæ®ã€‚
       - ä¿æŒç®€æ´ï¼Œæ¯æ¡çŸ¥è¯†ç‚¹ 50-100 å­—ã€‚
    
    è¾“å‡ºæ ¼å¼ç¤ºä¾‹ï¼š
    {{
        "title": "PyTorch æ ¸å¿ƒåŸç†è§£æ",
        "summary": "æœ¬æ–‡é€šè¿‡æ¯”å–»è¯¦ç»†è§£é‡Šäº† PyTorch ä¸­ç±»ä¸å‡½æ•°çš„åŒºåˆ«...",
        "tags": ["AI", "Python", "Deep Learning"],
        "key_points": [
            "Class (ç±») ç›¸å½“äºå›¾çº¸ï¼Œç”¨äºå­˜å‚¨æ¨¡å‹å‚æ•°ï¼ˆè®°å¿†ï¼‰ã€‚",
            "Def (å‡½æ•°) ç›¸å½“äºåŠ¨ä½œï¼Œç”¨äºå®šä¹‰å‰å‘ä¼ æ’­çš„è®¡ç®—æµç¨‹ã€‚",
            "__init__ æ˜¯åˆå§‹åŒ–é˜¶æ®µï¼Œforward æ˜¯æ¨ç†é˜¶æ®µã€‚"
        ]
    }}
    """
    
    # è°ƒç”¨ LLM
    response = get_completion(prompt)
    
    # === å…³é”®æ­¥éª¤ï¼šæ¸…æ´—å’Œè§£æ JSON ===
    # AI æœ‰æ—¶å€™ä¼šå› ä¸ºä¸ºäº†å¥½çœ‹åŠ ä¸Š markdown æ ‡è®° (```json ... ```)ï¼Œæˆ‘ä»¬éœ€è¦åˆ æ‰å®ƒ
    clean_json = response.replace("```json", "").replace("```", "").strip()
    
    try:
        data = json.loads(clean_json)
        return data
    except json.JSONDecodeError:
        print(f"âŒ JSON è§£æå¤±è´¥ï¼ŒåŸå§‹è¿”å›: {response}")
        # å¦‚æœè§£æå¤±è´¥ï¼Œåšä¸€ä¸ªå…œåº•è¿”å›ï¼Œé˜²æ­¢ç¨‹åºå´©æºƒ
        return {
            "title": "æœªå‘½åç¬”è®° (è§£æå¤±è´¥)", 
            "summary": response[:500],  # ç›´æ¥æŠŠ AI çš„å›å¤å½“æ‘˜è¦
            "tags": ["Error"],
            "key_points": ["è‡ªåŠ¨æ•´ç†å¤±è´¥ï¼Œè¯·æŸ¥çœ‹ summary"] 
        }

# --- ğŸ§  å¤§è„‘ Aï¼šè¥¿è¯­å¤„ç†å™¨ (å‡çº§ç‰ˆ) ---
def check_topic_match(new_text, existing_pages):
    titles_list = [f"ID: {p['id']}, Title: {p['title']}" for p in existing_pages]
    titles_str = "\n".join(titles_list) if titles_list else "æš‚æ— ç°æœ‰ç¬”è®°"
    
    prompt = f"""
    ä½ æ˜¯ä¸€ä¸ªæ™ºèƒ½å›¾ä¹¦ç®¡ç†å‘˜ã€‚
    ã€ç°æœ‰è¥¿è¯­ç¬”è®°åˆ—è¡¨ã€‘
    {titles_str}
    ã€æ–°ç´ æå‰1000å­—ã€‘
    {new_text[:1000]}
    ã€ä»»åŠ¡ã€‘åˆ¤æ–­æ–°ç´ æçš„æ ¸å¿ƒä¸»é¢˜æ˜¯å¦å·²å­˜åœ¨ï¼Ÿ
    è¾“å‡º JSON: 
    {{ "match": true, "page_id": "...", "page_title": "..." }} 
    æˆ– 
    {{ "match": false, "suggested_title": "..." }}
    """
    return get_completion(prompt)

def generate_spanish_content(raw_text):
    """
    æ ¸å¿ƒç”Ÿæˆé€»è¾‘ï¼šé’ˆå¯¹é•¿å­—å¹•è¿›è¡Œäº†ä¸“é—¨ä¼˜åŒ–
    """
    prompt = f"""
    ä½ æ˜¯ä¸€ä¸ªç²¾é€šè¥¿ç­ç‰™è¯­æ•™å­¦çš„é«˜çº§ç¼–è¾‘ã€‚
    ç”¨æˆ·çš„è¾“å…¥å¯èƒ½æ˜¯ä¸€ä»½ç¬”è®°ï¼Œä¹Ÿå¯èƒ½æ˜¯ä¸€æ®µé•¿è§†é¢‘çš„ã€å­—å¹•åŸæ–‡ã€‘ã€‚
    
    ã€è¾“å…¥å†…å®¹ã€‘
    {raw_text[:15000]}  <-- å…³é”®ä¿®æ”¹ï¼šè¯»å–å‰ 15000 å­—ï¼Œç¡®ä¿è¯»åˆ°å®Œæ•´å†…å®¹ï¼
    
    ã€ä½ çš„ä»»åŠ¡ã€‘
    ä¸è¦åªæ˜¯æ€»ç»“ï¼å¿…é¡»è¿›è¡Œ**â€œçŸ¥è¯†èƒå–â€**ã€‚
    
    1. **è¯†åˆ«æ ¸å¿ƒè¯æ±‡**ï¼šå¦‚æœè¾“å…¥æ˜¯å­—å¹•ï¼Œå¿…é¡»ä»ä¸­æå–å‡ºè‡³å°‘ 5-10 ä¸ªæ ¸å¿ƒç”Ÿè¯/çŸ­è¯­ï¼Œåˆ¶ä½œæˆè¡¨æ ¼ã€‚
    2. **æå–åœ°é“ä¾‹å¥**ï¼šæ‰¾åˆ°åŸæ–‡æœ¬ä¸­å‡ºç°çš„ç²¾å½©å¥å­ï¼Œä¿ç•™è¥¿è¯­åŸæ–‡å¹¶é™„å¸¦ä¸­æ–‡è§£é‡Šã€‚
    3. **ç»“æ„åŒ–é‡ç»„**ï¼šä½¿ç”¨ Heading, List, Table å°†å†…å®¹æ’ç‰ˆã€‚
    
    ã€JSON è¾“å‡ºç»“æ„ã€‘
    {{
        "title": "ç²¾å‡†çš„æ ‡é¢˜ (å¦‚æœæ˜¯è§†é¢‘ï¼Œç”¨è§†é¢‘ä¸»é¢˜)",
        "category": "å¬åŠ›/è¯æ±‡/è¯­æ³•",
        "summary": "200å­—å·¦å³çš„å†…å®¹ç®€ä»‹ï¼Œæ¦‚æ‹¬è§†é¢‘ä¸»è¦åœºæ™¯å’Œè¯é¢˜",
        "blocks": [
            {{ "type": "heading", "content": "1. æ ¸å¿ƒè¯æ±‡ (Vocabulario)" }},
            {{ 
                "type": "table", 
                "content": {{
                    "headers": ["è¥¿è¯­", "ä¸­æ–‡å«ä¹‰", "å¤‡æ³¨/åœºæ™¯"],
                    "rows": [
                        ["el cajero", "æ”¶é“¶å‘˜", "å•†åœºç»“è´¦åœºæ™¯"],
                        ["pagar con tarjeta", "åˆ·å¡æ”¯ä»˜", "å¸¸ç”¨çŸ­è¯­"]
                    ]
                }}
            }},
            {{ "type": "heading", "content": "2. ç²¾å½©ä¾‹å¥ (Frases Clave)" }},
            {{ "type": "list", "content": ["Me gustarÃ­a probarme esta camisa. (æˆ‘æƒ³è¯•è¯•è¿™ä»¶è¡¬è¡«)", "Â¿Tienen este pantalÃ³n en talla M? (è¿™æ¡è£¤å­æœ‰Mç å—?)"] }},
            {{ "type": "heading", "content": "3. è¯­æ³•/æ–‡åŒ–è¦ç‚¹" }},
            {{ "type": "text", "content": "è¿™é‡Œæ”¾è¯­æ³•è§£ææˆ–æ–‡åŒ–èƒŒæ™¯..." }}
        ]
    }}
    
    åªè¾“å‡º JSONã€‚
    """
    return get_completion(prompt)

def decide_merge_strategy(new_text, existing_structure_text, available_tables):
    prompt = f"""
    ä½ æ˜¯ä¸€ä¸ªæ™ºèƒ½ç¼–è¾‘ã€‚
    ã€é¡µé¢ç»“æ„ã€‘{existing_structure_text}
    ã€ç°æœ‰è¡¨æ ¼ã€‘{json.dumps(available_tables)}
    ã€æ–°å†…å®¹ã€‘{new_text[:2000]}
    ä»»åŠ¡ï¼šåˆ¤æ–­æ˜¯å¦é€‚åˆæ’å…¥ç°æœ‰è¡¨æ ¼ï¼Ÿ
    è¾“å‡º JSON: {{ "action": "insert_row", "table_id": "...", "row_data": [...] }} æˆ– {{ "action": "append_text" }}
    """
    return get_completion(prompt)

# --- ğŸ© æ€»æŒ‡æŒ¥é€»è¾‘ (å‡çº§ç‰ˆ) ---
def main_workflow(user_input=None, uploaded_file=None):
    """
    å…¥å£æ”¯æŒä¸¤ç§æ¨¡å¼ï¼š
    1. user_input: æ–‡æœ¬æˆ– URL
    2. uploaded_file: Streamlit çš„æ–‡ä»¶å¯¹è±¡
    """
    processed_text = ""
    original_url = None
    
    # === æƒ…å†µ A: å¤„ç†æ–‡ä»¶ä¸Šä¼  ===
    if uploaded_file:
        print("ğŸ“‚ æ£€æµ‹åˆ°æ–‡ä»¶è¾“å…¥...")
        content = read_pdf_content(uploaded_file)
        if not content: return
        processed_text = content

    # === æƒ…å†µ B: å¤„ç†æ–‡æœ¬/URL è¾“å…¥ ===
    elif user_input:
        # 1. è¯†åˆ« URL
        url_pattern = re.compile(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')
        if url_pattern.match(user_input.strip()):
            original_url = user_input.strip()
            print(f"ğŸŒ æ­£åœ¨æŠ“å– URL: {original_url}")
            content = fetch_url_content(original_url)
            if not content: return
            processed_text = f"ã€æ¥æº URLã€‘{original_url}\n\n{content}"
        else:
            processed_text = user_input
    
    else:
        print("âš ï¸ æ²¡æœ‰æ”¶åˆ°ä»»ä½•è¾“å…¥")
        return

    # === ä¸‹é¢æµç¨‹é€šç”¨ (è·¯ç”± -> ç”Ÿæˆ -> å…¥åº“) ===
    
    # 2. ğŸš¦ è·¯ç”±åˆ†ç±»
    print("ğŸš¦ æ­£åœ¨åˆ†æå†…å®¹ç±»å‹ (è·¯ç”±ä¸­)...")
    intent = classify_intent(processed_text)
    content_type = intent.get('type', 'General')
    print(f"ğŸ‘‰ åˆ¤å®šç±»å‹ä¸ºï¼šã€{content_type}ã€‘")


    # === é€šé“ A: è¥¿è¯­å­¦ä¹  ===
    if content_type == 'Spanish':
        
        # ä¸ºäº†æ¼”ç¤ºç®€å•ï¼Œè¿™é‡Œå‡è®¾ all_pages ä»…æ¥è‡ªè¥¿è¯­åº“
        # å®é™…é¡¹ç›®ä¸­åº”ä¼ å…¥ DB_SPANISH
        all_pages = get_all_page_titles() 
        
        match_result = parse_json(check_topic_match(processed_text, all_pages))
        
        if match_result and match_result.get('match'):
            page_id = match_result.get('page_id')
            page_title = match_result.get('page_title', 'æœªçŸ¥æ ‡é¢˜')
            print(f"ğŸ’¡ Merging with existing note: ã€Š{page_title}ã€‹")
            
            if not page_id: return

            structure_text, tables = get_page_structure(page_id)
            
            if tables:
                merge_decision = parse_json(decide_merge_strategy(processed_text, structure_text, tables))
                if merge_decision and merge_decision.get('action') == 'insert_row':
                    print(" â• Inserting table row...")
                    add_row_to_table(merge_decision['table_id'], merge_decision['row_data'])
                else:
                    print(" â• Appending content...")
                    full_content = parse_json(generate_spanish_content(processed_text))
                    if full_content:
                        append_to_page(page_id, full_content['summary'], full_content['blocks'])
            else:
                full_content = parse_json(generate_spanish_content(processed_text))
                if full_content:
                    append_to_page(page_id, full_content['summary'], full_content['blocks'])
        else:
            print("ğŸ†• Creating new Spanish note...")
            full_content = parse_json(generate_spanish_content(processed_text))
            if full_content:
                create_study_note(
                    full_content['title'], 
                    full_content.get('category', 'å¬åŠ›'), # å­—å¹•é€šå¸¸å½’ç±»ä¸ºå¬åŠ›
                    full_content['summary'], 
                    full_content['blocks']
                )

    # === é€šé“ B: é€šç”¨çŸ¥è¯† ===
    else:
        print("ğŸŒ Entering General Knowledge Mode...")
        if not DB_GENERAL:
            print("âŒ é”™è¯¯ï¼šæœªé…ç½®é€šç”¨æ•°æ®åº“ ID")
            return

        analysis = parse_json(process_general_knowledge(processed_text))
        if analysis:
            create_general_note(
                title=analysis.get('title', 'æœªå‘½åæ–‡æ¡£'),
                tags=analysis.get('tags', []),
                summary=analysis.get('summary', ''),
                url=original_url,
                content_blocks=analysis.get('blocks', []),
                db_id=DB_GENERAL
            )

if __name__ == "__main__":
    raw = read_input_file()
    if raw and "Enter content here" not in raw:
        main_workflow(raw)
        print("\nğŸ‰ Processing Complete!")